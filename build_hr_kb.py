from docx import Document
import json
from openai import OpenAI
import os
import re
from datetime import datetime, timezone
from pathlib import Path

client = OpenAI()

POLICIES_DIR = Path("policies")
OUTPUT_PATH = "hr_kb_index.json"

# æª”åæ ¼å¼ï¼šHR-103-03_å‡ºå‹¤ç®¡ç†è¾¦æ³•_202509.docxã€QP-212-07_åœ‹å…§å¤–å‡ºå·®ç®¡ç†è¾¦æ³•_202509.docx
FILENAME_RE = re.compile(r"^([A-Z]{2}-\d{3}-\d{2})_(.+)_(\d{6})\.docx$", re.IGNORECASE)


def chunk_text(text: str, chunk_size: int = 700, overlap: int = 120):
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    if not text:
        return []
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + chunk_size)
        chunks.append(text[start:end])
        if end == n:
            break
        start = max(0, end - overlap)
    return chunks


def read_docx_text(path: Path) -> str:
    doc = Document(str(path))
    parts = []

    # æ®µè½
    for p in doc.paragraphs:
        t = (p.text or "").strip()
        if t:
            parts.append(t)

    # è¡¨æ ¼ï¼ˆé‡è¦ï¼šå¾ˆå¤šè¦ç« çš„æ¢æ¬¾/è¡¨å–®åœ¨è¡¨æ ¼ï¼‰
    for table in doc.tables:
        for row in table.rows:
            cells = [(c.text or "").strip() for c in row.cells]
            cells = [c for c in cells if c]
            if cells:
                parts.append(" | ".join(cells))

    return "\n".join(parts)


def parse_meta_from_filename(filename: str):
    m = FILENAME_RE.match(filename)
    if m:
        policy_code = m.group(1).upper()
        policy_name = m.group(2).strip()
        policy_month = m.group(3)
    else:
        policy_code = "æœªçŸ¥ç‰ˆæ¬¡"
        policy_name = Path(filename).stem
        policy_month = "æœªçŸ¥æœˆä»½"
    return policy_month, policy_code, policy_name


def embed_text(text: str):
    emb = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return emb.data[0].embedding


def main():
    if not POLICIES_DIR.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™å¤¾ï¼š{POLICIES_DIR}ï¼ˆè«‹ç¢ºèª repo å…§æœ‰ /policiesï¼‰")

    policy_files = sorted(
        [p for p in POLICIES_DIR.glob("*.docx") if p.name.lower() != ".gitkeep"],
        key=lambda p: p.name
    )

    if not policy_files:
        raise FileNotFoundError("policies/ å…§æ‰¾ä¸åˆ°ä»»ä½• .docxï¼ˆè«‹æŠŠè¦ç«  .docx ä¸Šå‚³åˆ° /policiesï¼‰")

    meta = {
        "generated_at_utc": datetime.now(timezone.utc).isoformat(),
        "policies_count": len(policy_files),
        "policies_dir": str(POLICIES_DIR),
        "schema": "hr_kb_index_v1"
    }

    items = []
    policy_summaries = []

    for path in policy_files:
        filename = path.name
        policy_month, policy_code, policy_name = parse_meta_from_filename(filename)

        full_text = read_docx_text(path)
        chunks = chunk_text(full_text)

        policy_summaries.append({
            "source_filename": filename,
            "policy_month": policy_month,
            "policy_code": policy_code,
            "policy_name": policy_name,
            "chunks": len(chunks),
        })

        for idx, ch in enumerate(chunks, start=1):
            item = {
                "source_filename": filename,
                "policy_month": policy_month,
                "policy_code": policy_code,
                "policy_name": policy_name,
                "chunk_id": idx,
                "text": ch
            }
            item["embedding"] = embed_text(ch)
            items.append(item)

        print(f"âœ… {filename} -> {len(chunks)} chunks")

    output = {
        "meta": meta,
        "policies": policy_summaries,
        "items": items
    }

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"ğŸ‰ å»ºåº«å®Œæˆï¼š{OUTPUT_PATH}")
    print(f"ç¸½ chunksï¼š{len(items)}")


if __name__ == "__main__":
    main()
