from docx import Document
import json
import os
import re
from datetime import datetime, timezone
from openai import OpenAI

client = OpenAI()

POLICIES_DIR = "policies"
OUTPUT_KB_PATH = "hr_kb.json"
OUTPUT_INDEX_PATH = "hr_kb_index.json"

# æª”åæ ¼å¼ï¼šHR-103-03_å‡ºå‹¤ç®¡ç†è¾¦æ³•_202509.docx
#         QP-212-07_åœ‹å…§å¤–å‡ºå·®ç®¡ç†è¾¦æ³•_202509.docx
FILENAME_RE = re.compile(r"^([A-Za-z]+-\d+-\d+)_([^_]+)_(\d{6})\.docx$")


def parse_filename(filename: str):
    base = os.path.basename(filename)
    m = FILENAME_RE.match(base)
    if not m:
        return {
            "policy_code": "æœªçŸ¥ç‰ˆæ¬¡",
            "policy_name": "æœªçŸ¥è¾¦æ³•",
            "policy_month": "æœªçŸ¥æœˆä»½",
            "source_filename": base,
        }
    return {
        "policy_code": m.group(1),
        "policy_name": m.group(2),
        "policy_month": m.group(3),
        "source_filename": base,
    }


def read_docx_text(path: str) -> str:
    doc = Document(path)
    parts = []

    # æ®µè½
    for p in doc.paragraphs:
        t = (p.text or "").strip()
        if t:
            parts.append(t)

    # è¡¨æ ¼ï¼ˆæŠŠæ¯ä¸€åˆ—ä¸²æˆä¸€è¡Œï¼‰
    for table in doc.tables:
        for row in table.rows:
            cells = [(c.text or "").strip() for c in row.cells]
            line = " | ".join([c for c in cells if c])
            if line.strip():
                parts.append(line)

    return "\n".join(parts)


def chunk_text(text: str, chunk_size=650, overlap=120):
    """
    ç”¨å­—å…ƒé•·åº¦åˆ‡ chunkï¼ˆå°ä¸­æ–‡ç©©å®šï¼‰ï¼Œä¸¦ä¿ç•™ overlap
    """
    text = (text or "").strip()
    if not text:
        return []

    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(start + chunk_size, n)
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        if end == n:
            break
        start = max(0, end - overlap)
    return chunks


def list_policy_docx_files():
    if not os.path.isdir(POLICIES_DIR):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™å¤¾: ./{POLICIES_DIR}")

    files = []
    for name in os.listdir(POLICIES_DIR):
        if name.startswith("."):
            continue
        if not name.lower().endswith(".docx"):
            continue
        files.append(os.path.join(POLICIES_DIR, name))

    # æ’åºï¼šæœˆä»½æ–° -> èˆŠï¼ˆå…¶æ¬¡æª”åï¼‰
    def sort_key(p):
        meta = parse_filename(p)
        month = meta["policy_month"]
        # æœªçŸ¥æœˆä»½æ”¾æœ€å¾Œ
        month_key = month if re.match(r"^\d{6}$", month) else "000000"
        return (month_key, meta["source_filename"])

    files.sort(key=sort_key, reverse=True)
    return files


def main():
    docx_files = list_policy_docx_files()
    if not docx_files:
        raise FileNotFoundError(f"./{POLICIES_DIR} å…§æ‰¾ä¸åˆ°ä»»ä½• .docx")

    generated_at = datetime.now(timezone.utc).isoformat()

    policies = []
    items = []

    for path in docx_files:
        meta = parse_filename(path)

        # ç”¨ code+month ç•¶ policy_idï¼ˆæ–¹ä¾¿ app.py é¡¯ç¤º/ç¯©é¸ï¼‰
        policy_id = f'{meta["policy_code"]}_{meta["policy_month"]}'
        meta["policy_id"] = policy_id
        meta["source_path"] = path.replace("\\", "/")
        meta["updated_at_utc"] = generated_at
        policies.append(meta)

        full_text = read_docx_text(path)
        chunks = chunk_text(full_text)

        print(f'ğŸ“„ {meta["source_filename"]} -> {len(chunks)} chunks')

        for idx, ch in enumerate(chunks, start=1):
            emb = client.embeddings.create(
                model="text-embedding-3-small",
                input=ch
            )
            items.append({
                "policy_id": policy_id,
                "policy_code": meta["policy_code"],
                "policy_name": meta["policy_name"],
                "policy_month": meta["policy_month"],
                "source_filename": meta["source_filename"],
                "chunk_id": idx,
                "text": ch,
                "embedding": emb.data[0].embedding,
            })

    # è¼¸å‡º index
    index_out = {
        "generated_at_utc": generated_at,
        "policies_dir": POLICIES_DIR,
        "policies": policies,
    }
    with open(OUTPUT_INDEX_PATH, "w", encoding="utf-8") as f:
        json.dump(index_out, f, ensure_ascii=False, indent=2)

    # è¼¸å‡º kb
    kb_out = {
        "meta": {
            "generated_at_utc": generated_at,
            "policies_dir": POLICIES_DIR,
            "policy_count": len(policies),
            "item_count": len(items),
        },
        "policies": policies,   # æ–¹ä¾¿ app.py ç›´æ¥å–ç”¨
        "items": items,
    }
    with open(OUTPUT_KB_PATH, "w", encoding="utf-8") as f:
        json.dump(kb_out, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²è¼¸å‡ºï¼š{OUTPUT_INDEX_PATH}, {OUTPUT_KB_PATH}")


if __name__ == "__main__":
    main()
