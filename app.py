# ===== import å€åŸŸ =====
from flask import Flask, request, abort
import os
import time
import json
import math
import re
from collections import Counter
from openai import OpenAI

from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, TextMessage, TextSendMessage
# ===== import å€åŸŸçµæŸ =====


# ===== å¤šèªç³»æ–‡å­—ï¼ˆå›ºå®šå­—ä¸²ï¼Œä¸äº¤çµ¦ GPTï¼‰=====
TEXTS = {
    "zh": {
        "intro": (
            "ä½ å¥½ï¼Œæˆ‘æ˜¯ã€å¾®è½‰äººè³‡AIåŠ©æ‰‹ã€‘ğŸ¤–\n\n"
            "ä½ å¯ä»¥ç›´æ¥è¼¸å…¥äººè³‡ç›¸é—œå•é¡Œï¼Œä¾‹å¦‚ï¼š\n"
            "ãƒ»ç—…å‡è¦å‰‡æ˜¯ä»€éº¼ï¼Ÿ\n"
            "ãƒ»åŠ ç­è²»å¦‚ä½•è¨ˆç®—ï¼Ÿ\n"
            "ãƒ»ç‰¹ä¼‘è¦å®šæœ‰å“ªäº›ï¼Ÿ\n"
        ),
        "fallback": "æ­¤å•é¡Œåœ¨ç›®å‰è¦ç« å¼•ç”¨å…§å®¹ä¸­æ‰¾ä¸åˆ°æ˜ç¢ºä¾æ“šï¼Œè«‹æ´½äººè³‡å°ˆå“¡ã€‚",
        "answer_label": "ã€HR AI åŠ©æ‰‹å›è¦†ã€‘\n\n",
        "disclaimer": (
            "\n\nâ€”â€”â€”\n"
            "âš ï¸ æœ¬å›è¦†ç”±å¾®è½‰äººè³‡ AI åŠ©ç†ä¾æ“šç¾è¡Œå…§éƒ¨è¦ç« æ–‡ä»¶æ‘˜éŒ„æä¾›ï¼Œåƒ…ä¾›åƒè€ƒã€‚\n"
            "å¦‚æœ‰ç–‘æ…®æˆ–éœ€æ­£å¼èªå®šï¼Œè«‹æ´½äººè³‡å°ˆå“¡ä¸¦ä»¥å…¬å¸å…¬å‘Šç‚ºæº–ã€‚"
        ),
        "lang_rule": "è«‹ç”¨ç¹é«”ä¸­æ–‡å›è¦†ã€‚",
        "prefix_main": "ğŸ“Œ ä¸»è¦ä¾æ“šï¼š{month} çš„ {code} ç‰ˆæœ¬ã€Š{name}ã€‹\n",
        "prefix_others": "ğŸ“ å¦åƒè€ƒï¼š{others}\n\n",
    },
    "en": {
        "intro": (
            "Hi, I'm the microSHIFT HR AI assistant ğŸ¤–\n\n"
            "You can ask HR-related questions, for example:\n"
            "â€¢ Sick leave policy\n"
            "â€¢ Overtime pay calculation\n"
            "â€¢ Annual leave rules\n"
        ),
        "fallback": "I can't find a clear basis in the current internal policy excerpts. Please contact HR.",
        "answer_label": "[HR AI Assistant]\n\n",
        "disclaimer": (
            "\n\nâ€”â€”â€”\n"
            "âš ï¸ This reply is generated by an HR AI assistant based on internal policy excerpts for reference only.\n"
            "For official confirmation, please contact HR and refer to company announcements."
        ),
        "lang_rule": "Please reply in English.",
        "prefix_main": "ğŸ“Œ Primary source: {month} version {code} â€œ{name}â€\n",
        "prefix_others": "ğŸ“ Additional references: {others}\n\n",
    },
    "vi": {
        "intro": (
            "Xin chÃ o, tÃ´i lÃ  trá»£ lÃ½ AI NhÃ¢n sá»± microSHIFT ğŸ¤–\n\n"
            "Báº¡n cÃ³ thá»ƒ há»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n NhÃ¢n sá»±, vÃ­ dá»¥:\n"
            "â€¢ Quy Ä‘á»‹nh nghá»‰ á»‘m\n"
            "â€¢ CÃ¡ch tÃ­nh lÆ°Æ¡ng tÄƒng ca\n"
            "â€¢ Quy Ä‘á»‹nh nghá»‰ phÃ©p nÄƒm\n"
        ),
        "fallback": "TÃ´i khÃ´ng tÃ¬m tháº¥y cÄƒn cá»© rÃµ rÃ ng trong trÃ­ch Ä‘oáº¡n quy Ä‘á»‹nh hiá»‡n táº¡i. Vui lÃ²ng liÃªn há»‡ NhÃ¢n sá»±.",
        "answer_label": "[Trá»£ lÃ½ AI NhÃ¢n sá»±]\n\n",
        "disclaimer": (
            "\n\nâ€”â€”â€”\n"
            "âš ï¸ CÃ¢u tráº£ lá»i nÃ y do trá»£ lÃ½ AI NhÃ¢n sá»± táº¡o dá»±a trÃªn trÃ­ch Ä‘oáº¡n quy Ä‘á»‹nh ná»™i bá»™ vÃ  chá»‰ mang tÃ­nh tham kháº£o.\n"
            "Äá»ƒ xÃ¡c nháº­n chÃ­nh thá»©c, vui lÃ²ng liÃªn há»‡ NhÃ¢n sá»± vÃ  theo thÃ´ng bÃ¡o cá»§a cÃ´ng ty."
        ),
        "lang_rule": "Vui lÃ²ng tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.",
        "prefix_main": "ğŸ“Œ Nguá»“n chÃ­nh: phiÃªn báº£n {month} {code} â€œ{name}â€\n",
        "prefix_others": "ğŸ“ Tham kháº£o thÃªm: {others}\n\n",
    },
}


def detect_lang(text: str) -> str:
    """
    ç°¡æ˜“èªè¨€åˆ¤æ–·ï¼ˆä¸é¡å¤–å‘¼å« GPTï¼ŒçœéŒ¢ä¹Ÿæ›´ç©©ï¼‰ï¼š
    - æœ‰è¶Šå—æ–‡è®ŠéŸ³ç¬¦è™Ÿ -> vi
    - è‹±æ–‡å­—æ¯æ¯”ä¾‹è¼ƒé«˜ -> en
    - å…¶ä»– -> zh
    """
    t = (text or "").strip()
    if not t:
        return "zh"

    # è¶Šå—æ–‡å¸¸è¦‹è®ŠéŸ³ç¬¦è™Ÿï¼ˆç²—ç•¥ï¼‰
    if re.search(r"[Ã Ã¡Ã¢Ã£Äƒáº¡áº£áº¥áº§áº©áº«áº­áº¯áº±áº³áºµáº·Ã¨Ã©Ãªáº¹áº»áº½áº¿á»á»ƒá»…á»‡Ã¬Ã­Ã®Ä©á»‹Ã²Ã³Ã´ÃµÆ¡á»á»á»‘á»“á»•á»—á»™á»›á»á»Ÿá»¡á»£Ã¹ÃºÃ»Å©Æ°á»¥á»§á»©á»«á»­á»¯á»±á»³Ã½á»µá»·á»¹Ä‘]",
                 t, re.IGNORECASE):
        return "vi"

    letters = re.findall(r"[A-Za-z]", t)
    if len(letters) >= 6 and (len(letters) / max(len(t), 1)) > 0.25:
        return "en"

    return "zh"


# ===== HR Bot è¨­å®š =====
INTRO_COOLDOWN_SECONDS = 60 * 60 * 12  # 12 å°æ™‚
last_seen = {}  # æš«å­˜æ¯å€‹ LINE ä½¿ç”¨è€…çš„æœ€å¾Œäº’å‹•æ™‚é–“ï¼ˆRender é‡å•Ÿæœƒæ¸…ç©ºï¼Œæ­£å¸¸ï¼‰

SIMILARITY_THRESHOLD = 0.28  # å¤ªå¸¸æ‹’ç­”å¯å¾®èª¿ 0.24~0.26ï¼›å¤ªå®¹æ˜“äº‚ç­”å¯èª¿é«˜
TOP_K = 6  # æŠ“å›çš„å¼•ç”¨æ®µè½æ•¸

# ===== è¿½å•ç‹€æ…‹ï¼ˆæ–°å¢ï¼‰=====
FOLLOWUP_TTL_SECONDS = 60 * 30  # 30 åˆ†é˜å…§è¦–ç‚ºåŒä¸€æ®µå°è©±å¯è¿½å•
followup_state = {}  # user_id -> {"last_q": str, "last_t": float}


def is_followup(user_text: str, lang: str, prev_q: str, prev_t: float) -> bool:
    """
    è¿½å•åˆ¤å®šï¼ˆä¸äº¤çµ¦ GPTï¼‰ï¼š
    - éœ€æœ‰ä¸Šä¸€é¡Œï¼Œä¸”åœ¨ TTL å…§
    - æ–‡å­—åçŸ­ / å¸¸è¦‹è¿½å•é–‹é ­ / ä»£åè©æŒ‡æ¶‰ ç­‰
    """
    if not prev_q or not prev_t:
        return False

    now = time.time()
    if (now - prev_t) > FOLLOWUP_TTL_SECONDS:
        return False

    t = (user_text or "").strip()
    if not t:
        return False

    # å¤ªé•·é€šå¸¸å·²ç¶“æ˜¯æ–°å•é¡Œï¼ˆå¯è‡ªè¡Œèª¿æ•´ï¼‰
    if lang == "zh":
        if len(t) <= 18:
            return True
        follow_starts = ("é‚£", "æ‰€ä»¥", "å¦å¤–", "é‚„æœ‰", "å†å•", "é‚£å¦‚æœ", "é‚£æœƒ", "é‚£æ˜¯ä¸æ˜¯", "æœƒä¸æœƒ", "æ˜¯å¦", "å¯ä»¥å—", "æ€éº¼ç®—")
        refer_words = ("é€™å€‹", "é‚£å€‹", "å®ƒ", "ä¸Šè¿°", "å‰é¢", "å‰›å‰›", "ä¸Šé¢", "å‰›æ‰", "åŒæ¨£", "ä¸€æ¨£")
        if t.startswith(follow_starts) or any(w in t for w in refer_words):
            return True
        # å¾ˆçŸ­ä¸”æ˜¯å•å¥
        if len(t) <= 25 and ("å—" in t or "?" in t or "ï¼Ÿ" in t):
            return True

    elif lang == "en":
        if len(t) <= 30:
            return True
        follow_starts = ("what about", "and", "also", "so", "then", "in that case", "does it", "do I", "will it", "is it")
        if any(t.lower().startswith(s) for s in follow_starts):
            return True

    else:  # vi
        if len(t) <= 35:
            return True
        follow_starts = ("cÃ²n", "váº­y", "tháº¿", "náº¿u", "vÃ ", "cÃ³", "cÃ³ bá»‹", "cÃ³ Ä‘Æ°á»£c", "thÃ¬")
        if any(t.lower().startswith(s) for s in follow_starts):
            return True

    return False


# ===== å»ºç«‹ Flask / OpenAI / LINE ç‰©ä»¶ =====
app = Flask(__name__)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
line_bot_api = LineBotApi(os.getenv("LINE_CHANNEL_ACCESS_TOKEN"))
handler = WebhookHandler(os.getenv("LINE_CHANNEL_SECRET"))

# ===== è¼‰å…¥ HR KB Indexï¼ˆå•Ÿå‹•æ™‚è®€ä¸€æ¬¡ï¼‰=====
KB_INDEX_PATH = os.path.join(os.path.dirname(__file__), "hr_kb_index.json")
with open(KB_INDEX_PATH, "r", encoding="utf-8") as f:
    KB = json.load(f)

KB_ITEMS = KB.get("items", [])


# ===== å‘é‡ç›¸ä¼¼åº¦ =====
def cosine_sim(a, b):
    dot = 0.0
    na = 0.0
    nb = 0.0
    for x, y in zip(a, b):
        dot += x * y
        na += x * x
        nb += y * y
    if na == 0 or nb == 0:
        return 0.0
    return dot / (math.sqrt(na) * math.sqrt(nb))


def retrieve_chunks(query: str, top_k: int = TOP_K):
    # 1) query embedding
    q_emb = client.embeddings.create(
        model="text-embedding-3-small",
        input=query
    ).data[0].embedding

    # 2) compute similarity
    scored = []
    for it in KB_ITEMS:
        emb = it.get("embedding")
        txt = (it.get("text") or "").strip()
        if not emb or not txt:
            continue
        s = cosine_sim(q_emb, emb)
        scored.append((s, it))

    scored.sort(key=lambda x: x[0], reverse=True)
    top = scored[:top_k]

    # 3) å›å‚³ refsï¼ˆå«è¦ç« è³‡è¨Šï¼‰
    refs = []
    for s, it in top:
        refs.append({
            "score": float(s),
            "policy_month": it.get("policy_month", "æœªçŸ¥æœˆä»½"),
            "policy_code": it.get("policy_code", "æœªçŸ¥ç‰ˆæ¬¡"),
            "policy_name": it.get("policy_name", "æœªçŸ¥è¾¦æ³•"),
            "source_filename": it.get("source_filename", ""),
            "chunk_id": it.get("chunk_id", 0),
            "text": (it.get("text") or "").strip(),
        })
    return refs


def collect_policies_in_order(refs):
    """
    ä¾ç…§ refsï¼ˆç›¸ä¼¼åº¦ç”±é«˜åˆ°ä½ï¼‰æ”¶é›†ä¸é‡è¤‡çš„è¦ç« æ¸…å–®
    å›å‚³ [(month, code, name), ...] ç¬¬ä¸€å€‹è¦–ç‚ºä¸»è¦ä¾æ“š
    """
    seen = set()
    policies = []
    for r in refs:
        key = (r.get("policy_month"), r.get("policy_code"), r.get("policy_name"))
        if key in seen:
            continue
        seen.add(key)
        policies.append(key)
    return policies


def build_multi_source_prefix(T, refs):
    """
    ç”¢ç”Ÿå¤šä¾†æºå‰ç¶´ï¼š
    - ä¸»è¦ä¾æ“šï¼šrefs ä¸­æœ€é«˜åˆ†é‚£æ‰¹æ‰€å±¬è¦ç« ï¼ˆç¬¬ä¸€å€‹ï¼‰
    - å¦åƒè€ƒï¼šåŒä¸€æ¬¡æª¢ç´¢ä¸­å‡ºç¾çš„å…¶ä»–è¦ç« ï¼ˆå»é‡ï¼‰
    """
    policies = collect_policies_in_order(refs)
    if not policies:
        return "\n"

    main_month, main_code, main_name = policies[0]
    prefix = T["prefix_main"].format(month=main_month, code=main_code, name=main_name)

    if len(policies) > 1:
        others = "ã€".join([f"{m} {c}ã€Š{n}ã€‹" for m, c, n in policies[1:]])
        prefix += T["prefix_others"].format(others=others)
    else:
        prefix += "\n"

    return prefix


# ===== callback Webhook æ¥æ”¶ =====
@app.route("/callback", methods=["POST"])
def callback():
    signature = request.headers.get("X-Line-Signature")
    body = request.get_data(as_text=True)

    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)

    return "OK"


# ===== handlerï¼šHR + RAG + GPT =====
@handler.add(MessageEvent, message=TextMessage)
def handle_message(event):
    user_text = (event.message.text or "").strip()
    user_id = event.source.user_id

    # ---- èªè¨€åˆ¤æ–·ï¼ˆå›ºå®šè¦å‰‡ï¼‰----
    lang = detect_lang(user_text)
    T = TEXTS.get(lang, TEXTS["zh"])

    # ---- intro é¡¯ç¤ºç¯€å¥ ----
    now = time.time()
    last = last_seen.get(user_id)
    should_show_intro = (last is None) or ((now - last) > INTRO_COOLDOWN_SECONDS)
    last_seen[user_id] = now

    # ---- è¿½å•åˆ¤å®šï¼ˆæ–°å¢ï¼‰----
    prev = followup_state.get(user_id, {})
    prev_q = prev.get("last_q", "")
    prev_t = prev.get("last_t", 0.0)

    follow = is_followup(user_text, lang, prev_q, prev_t)

    if follow:
        if lang == "zh":
            effective_question = f"{prev_q}\nï¼ˆè¿½å•ï¼‰{user_text}"
        elif lang == "en":
            effective_question = f"{prev_q}\n(Follow-up) {user_text}"
        else:
            effective_question = f"{prev_q}\n(CÃ¢u há»i tiáº¿p) {user_text}"
    else:
        effective_question = user_text

    # æ›´æ–°è¿½å•ç‹€æ…‹ï¼šæŠŠã€Œæœ‰æ•ˆå•é¡Œã€ç•¶æˆä¸‹ä¸€è¼ªçš„ä¸Šä¸‹æ–‡
    #ï¼ˆé¿å…é€£çºŒè¿½å•æ™‚åªè¨˜å¾—å¾ˆçŸ­çš„ä¸€å¥ï¼‰
    followup_state[user_id] = {"last_q": effective_question, "last_t": now}

    # ---- RAG æª¢ç´¢ï¼ˆç”¨ effective_questionï¼‰----
    refs = retrieve_chunks(effective_question, top_k=TOP_K)
    best_score = refs[0]["score"] if refs else 0.0

    # ç›¸ä¼¼åº¦ä¸è¶³ï¼šç›´æ¥æ‹’ç­”ï¼ˆé¿å…æ³›åŒ–åˆ°å‹åŸºæ³•/ç¶²è·¯å¸¸è­˜ï¼‰
    if best_score < SIMILARITY_THRESHOLD:
        core = T["fallback"]
        if should_show_intro:
            reply_text = f"{T['intro']}\n{core}{T['disclaimer']}"
        else:
            reply_text = f"{core}{T['disclaimer']}"

        line_bot_api.reply_message(
            event.reply_token,
            TextSendMessage(text=reply_text)
        )
        return

    # ---- å¤šä¾†æºå‰ç¶´ï¼ˆåªæ”¹é€™è£¡å³å¯é¡¯ç¤ºå¤šè¦ç« ï¼‰----
    prefix = build_multi_source_prefix(T, refs)

    # ---- å¼•ç”¨å…§å®¹å€å¡Šï¼ˆåªçµ¦å¿…è¦ç‰‡æ®µï¼‰----
    context_block = "\n\n".join(
        [f"[{r['policy_code']}#{r['chunk_id']}] {r['text']}" for r in refs]
    )

    # ---- GPT Promptï¼ˆç¶­æŒä½ åŸæœ¬è¦å‰‡ï¼Œä¸é¡å¤–åŠ å…¥åˆ¶åº¦è§’è‰²èªªæ˜ï¼‰----
    prompt = f"""
[è¦å‰‡ï¼ˆéå¸¸é‡è¦ï¼‰]
1) ä½ åªèƒ½æ ¹æ“šä¸‹æ–¹ã€å¼•ç”¨å…§å®¹ã€‘å›ç­”ï¼Œç¦æ­¢ä½¿ç”¨ä¸€èˆ¬å¸¸è­˜ã€ç¶²è·¯è³‡è¨Šæˆ–æ¨æ¸¬è£œå……ã€‚
2) å¦‚æœã€å¼•ç”¨å…§å®¹ã€‘ä¸è¶³ä»¥å›ç­”ï¼Œè«‹åªå›è¦†ä¸‹åˆ—å¥å­ï¼ˆä¸è¦åŠ ä»»ä½•è£œå……ï¼‰ï¼š
{T['fallback']}

[èªè¨€è¦æ±‚]
{T['lang_rule']}

[å¼•ç”¨å…§å®¹]
{context_block}

[å“¡å·¥å•é¡Œ]
{effective_question}

[å›ç­”æ ¼å¼è¦æ±‚]
- å°ˆæ¥­ã€æ¸…æ¥šã€ç°¡çŸ­
- å„ªå…ˆç”¨æ¢åˆ—
- è‹¥æœ‰æ¢æ¬¾/æ¢ä»¶/ä¾‹å¤–ï¼Œéœ€è¬›æ¸…æ¥š
""".strip()

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯å…¬å¸å…§éƒ¨ HR Botï¼ˆåªèƒ½ä¾æ“šå¼•ç”¨å…§å®¹å›ç­”ï¼‰"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1
    )
    gpt_answer = (resp.choices[0].message.content or "").strip()

    # ---- çµ„åˆå›è¦†ï¼ˆå…è²¬è²æ˜å›ºå®šåŠ æœ€å¾Œï¼Œä¸äº¤çµ¦ GPTï¼‰----
    if should_show_intro:
        reply_text = f"{T['intro']}\n{prefix}{T['answer_label']}{gpt_answer}{T['disclaimer']}"
    else:
        reply_text = f"{prefix}{T['answer_label']}{gpt_answer}{T['disclaimer']}"

    line_bot_api.reply_message(
        event.reply_token,
        TextSendMessage(text=reply_text)
    )


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    app.run(host="0.0.0.0", port=port)
