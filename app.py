# ===== import å€åŸŸ =====
from flask import Flask, request, abort
import os
import time
import json
import math
import re
from collections import Counter
from openai import OpenAI

from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, TextMessage, TextSendMessage
# ===== import å€åŸŸçµæŸ =====


# ===== å¤šèªç³»æ–‡å­—ï¼ˆå›ºå®šå­—ä¸²ï¼Œä¸äº¤çµ¦ GPTï¼‰=====
TEXTS = {
    "zh": {
        "intro": (
            "ä½ å¥½ï¼Œæˆ‘æ˜¯ã€å¾®è½‰äººè³‡AIåŠ©æ‰‹ã€‘ğŸ¤–\n\n"
            "ä½ å¯ä»¥ç›´æ¥è¼¸å…¥äººè³‡ç›¸é—œå•é¡Œï¼Œä¾‹å¦‚ï¼š\n"
            "ãƒ»ç—…å‡è¦å‰‡æ˜¯ä»€éº¼ï¼Ÿ\n"
            "ãƒ»åŠ ç­è²»å¦‚ä½•è¨ˆç®—ï¼Ÿ\n"
            "ãƒ»ç‰¹ä¼‘è¦å®šæœ‰å“ªäº›ï¼Ÿ\n"
        ),
        "fallback": "æ­¤å•é¡Œåœ¨ç›®å‰è¦ç« å¼•ç”¨å…§å®¹ä¸­æ‰¾ä¸åˆ°æ˜ç¢ºä¾æ“šï¼Œè«‹æ´½äººè³‡å°ˆå“¡ã€‚",
        "answer_label": "ã€HR AI åŠ©æ‰‹å›è¦†ã€‘\n\n",
        "disclaimer": (
            "\n\nâ”€â”€â”€â”€\n"
            "âš ï¸ æœ¬å›è¦†ç”±å¾®è½‰äººè³‡ AI åŠ©ç†ä¾æ“šç¾è¡Œå…§éƒ¨è¦ç« æ–‡ä»¶æ‘˜éŒ„æä¾›ï¼Œåƒ…ä¾›åƒè€ƒã€‚\n"
            "å¦‚æœ‰ç–‘æ…®æˆ–éœ€æ­£å¼èªå®šï¼Œè«‹æ´½äººè³‡å°ˆå“¡ä¸¦ä»¥å…¬å¸å…¬å‘Šç‚ºæº–ã€‚"
        ),
        "lang_rule": "è«‹ç”¨ç¹é«”ä¸­æ–‡å›è¦†ã€‚",
        "prefix_main": "ğŸ“Œ ä¸»è¦ä¾æ“šï¼š{month} çš„ {code} ç‰ˆæœ¬ã€Š{name}ã€‹\n",
        "prefix_others": "ğŸ“ å¦åƒè€ƒï¼š{others}\n\n",
    },
    "en": {
        "intro": (
            "Hi, I'm the microSHIFT HR AI assistant ğŸ¤–\n\n"
            "You can ask HR-related questions, for example:\n"
            "â€¢ Sick leave policy\n"
            "â€¢ Overtime pay calculation\n"
            "â€¢ Annual leave rules\n"
        ),
        "fallback": "I can't find a clear basis in the current HR policy excerpts. Please contact HR.",
        "answer_label": "[HR AI Assistant]\n\n",
        "disclaimer": (
            "\n\nâ”€â”€â”€â”€\n"
            "âš ï¸ This reply is generated by an HR AI assistant based on internal policy excerpts for reference only.\n"
            "For official confirmation, please contact HR and refer to company announcements."
        ),
        "lang_rule": "Please reply in English.",
        "prefix_main": "ğŸ“Œ Primary source: {month} {code} â€œ{name}â€\n",
        "prefix_others": "ğŸ“ Additional references: {others}\n\n",
    },
    "vi": {
        "intro": (
            "Xin chÃ o, tÃ´i lÃ  trá»£ lÃ½ AI NhÃ¢n sá»± microSHIFT ğŸ¤–\n\n"
            "Báº¡n cÃ³ thá»ƒ há»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n NhÃ¢n sá»±, vÃ­ dá»¥:\n"
            "â€¢ Quy Ä‘á»‹nh nghá»‰ á»‘m\n"
            "â€¢ CÃ¡ch tÃ­nh lÆ°Æ¡ng tÄƒng ca\n"
            "â€¢ Quy Ä‘á»‹nh nghá»‰ phÃ©p nÄƒm\n"
        ),
        "fallback": "TÃ´i khÃ´ng tÃ¬m tháº¥y cÄƒn cá»© rÃµ rÃ ng trong trÃ­ch Ä‘oáº¡n quy Ä‘á»‹nh hiá»‡n táº¡i. Vui lÃ²ng liÃªn há»‡ NhÃ¢n sá»±.",
        "answer_label": "[Trá»£ lÃ½ AI NhÃ¢n sá»±]\n\n",
        "disclaimer": (
            "\n\nâ”€â”€â”€â”€\n"
            "âš ï¸ CÃ¢u tráº£ lá»i nÃ y do trá»£ lÃ½ AI NhÃ¢n sá»± táº¡o dá»±a trÃªn trÃ­ch Ä‘oáº¡n quy Ä‘á»‹nh ná»™i bá»™ vÃ  chá»‰ mang tÃ­nh tham kháº£o.\n"
            "Äá»ƒ xÃ¡c nháº­n chÃ­nh thá»©c, vui lÃ²ng liÃªn há»‡ NhÃ¢n sá»± vÃ  theo thÃ´ng bÃ¡o cá»§a cÃ´ng ty."
        ),
        "lang_rule": "Vui lÃ²ng tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.",
        "prefix_main": "ğŸ“Œ Nguá»“n chÃ­nh: phiÃªn báº£n {month} {code} â€œ{name}â€\n",
        "prefix_others": "ğŸ“ Tham kháº£o thÃªm: {others}\n\n",
    },
}


def detect_lang(text: str) -> str:
    """
    ç°¡æ˜“èªè¨€åˆ¤æ–·ï¼ˆä¸é¡å¤–å‘¼å« GPTï¼ŒçœéŒ¢ä¹Ÿæ›´ç©©ï¼‰ï¼š
    - æœ‰è¶Šå—æ–‡è®ŠéŸ³ç¬¦è™Ÿ â†’ vi
    - è‹±æ–‡å­—æ¯æ¯”ä¾‹åé«˜ â†’ en
    - å…¶ä»– â†’ zh
    """
    t = (text or "").strip()
    if not t:
        return "zh"

    # è¶Šå—æ–‡å¸¸è¦‹è®ŠéŸ³ç¬¦è™Ÿï¼ˆç²—ç•¥ä½†å¤ ç”¨ï¼‰
    if re.search(r"[ÄƒÃ¢Ä‘ÃªÃ´Æ¡Æ°Ã¡Ã áº£Ã£áº¡áº¥áº§áº©áº«áº­áº¯áº±áº³áºµáº·Ã©Ã¨áº»áº½áº¹áº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»á»‘á»“á»•á»—á»™á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µ]", t, re.IGNORECASE):
        return "vi"

    letters = re.findall(r"[A-Za-z]", t)
    if len(letters) >= 6 and (len(letters) / max(len(t), 1)) > 0.25:
        return "en"

    return "zh"


# ===== HR Bot è¨­å®š =====
INTRO_COOLDOWN_SECONDS = 60 * 60 * 12  # 12 å°æ™‚
last_seen = {}  # æš«å­˜æ¯å€‹ LINE ä½¿ç”¨è€…çš„æœ€å¾Œäº’å‹•æ™‚é–“ï¼ˆRender é‡å•Ÿæœƒæ¸…ç©ºï¼Œå±¬æ­£å¸¸ï¼‰

SIMILARITY_THRESHOLD = 0.28  # RAG ç›¸ä¼¼åº¦å¤ªä½å°±æ‹’ç­”ï¼ˆé¿å…æ³›å›ç­”ï¼‰
TOP_K = 6  # æŠ“å›ä¾†çš„å¼•ç”¨æ®µè½æ•¸

# ===== è¿½å•ï¼ˆFollow-upï¼‰è¨­å®šï¼šåªç”¨ Aï¼ˆèªæ„ç›¸ä¼¼åº¦ï¼‰=====
FOLLOWUP_TTL_SECONDS = 60 * 20      # å»ºè­° 20 åˆ†é˜å…§æ‰è¦–ç‚ºå¯èƒ½è¿½å•ï¼ˆæ¯” 12 å°æ™‚æ›´å®‰å…¨ï¼‰
FOLLOWUP_SIM_THRESHOLD = 0.72       # 0.70~0.78 é–“èª¿æ•´ï¼›è¶Šé«˜è¶Šä¸å®¹æ˜“èª¤ä¸²
followup_state = {}                 # user_id -> { last_q, last_t, last_emb }

# ===== å»ºç«‹ Flask / OpenAI / LINE ç‰©ä»¶ =====
app = Flask(__name__)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
line_bot_api = LineBotApi(os.getenv("LINE_CHANNEL_ACCESS_TOKEN"))
handler = WebhookHandler(os.getenv("LINE_CHANNEL_SECRET"))

# ===== è®€å– HR KB Indexï¼ˆå•Ÿå‹•æ™‚è®€ä¸€æ¬¡ï¼‰=====
KB_INDEX_PATH = os.path.join(os.path.dirname(__file__), "hr_kb_index.json")
with open(KB_INDEX_PATH, "r", encoding="utf-8") as f:
    KB = json.load(f)

KB_ITEMS = KB.get("items", [])


# ====== å‘é‡å·¥å…· ======
def cosine_sim(a, b):
    dot = 0.0
    na = 0.0
    nb = 0.0
    for x, y in zip(a, b):
        dot += x * y
        na += x * x
        nb += y * y
    if na == 0 or nb == 0:
        return 0.0
    return dot / (math.sqrt(na) * math.sqrt(nb))


def embed_text(s: str):
    """çµ±ä¸€ embedding å…¥å£ï¼Œæ–¹ä¾¿å¾ŒçºŒçœéŒ¢/èª¿æ•´æ¨¡å‹ã€‚"""
    return client.embeddings.create(
        model="text-embedding-3-small",
        input=s
    ).data[0].embedding


def retrieve_chunks(query: str, top_k: int = TOP_K, query_emb=None):
    """
    query_emb å¯é¸ï¼šè‹¥å¤–é¢å·²ç®—é embeddingï¼Œå°±ä¸è¦é‡ç®—ï¼ˆçœéŒ¢ï¼‰
    """
    q_emb = query_emb if query_emb is not None else embed_text(query)

    scored = []
    for it in KB_ITEMS:
        emb = it.get("embedding")
        txt = (it.get("text") or "").strip()
        if not emb or not txt:
            continue
        s = cosine_sim(q_emb, emb)
        scored.append((s, it))

    scored.sort(key=lambda x: x[0], reverse=True)
    top = scored[:top_k]

    refs = []
    for s, it in top:
        refs.append({
            "score": float(s),
            "policy_month": it.get("policy_month", "æœªçŸ¥æœˆä»½"),
            "policy_code": it.get("policy_code", "æœªçŸ¥ç‰ˆæ¬¡"),
            "policy_name": it.get("policy_name", "æœªçŸ¥è¾¦æ³•"),
            "source_filename": it.get("source_filename", ""),
            "chunk_id": it.get("chunk_id", 0),
            "text": (it.get("text", "") or "").strip()
        })
    return refs


def collect_policies_in_order(refs):
    """
    ä¾ç…§ refsï¼ˆç›¸ä¼¼åº¦ç”±é«˜åˆ°ä½ï¼‰æ”¶é›†ä¸é‡è¤‡çš„è¦ç« æ¸…å–®
    å›å‚³: [(month, code, name), ...] ç¬¬ä¸€å€‹è¦–ç‚ºä¸»è¦ä¾æ“š
    """
    seen = set()
    policies = []
    for r in refs:
        key = (r.get("policy_month"), r.get("policy_code"), r.get("policy_name"))
        if key in seen:
            continue
        seen.add(key)
        policies.append(key)
    return policies


def build_multi_source_prefix(T, refs):
    """
    ç”¢ç”Ÿå¤šä¾†æºå‰ç¶´ï¼š
    - ä¸»è¦ä¾æ“šï¼šrefs ä¸­æœ€é«˜åˆ†é‚£æ‰¹æ‰€å±¬è¦ç« ï¼ˆç¬¬ä¸€å€‹ï¼‰
    - å¦åƒè€ƒï¼šåŒä¸€æ¬¡æª¢ç´¢ä¸­å‡ºç¾çš„å…¶ä»–è¦ç« ï¼ˆå»é‡ï¼‰
    """
    policies = collect_policies_in_order(refs)
    if not policies:
        return "\n"

    main_month, main_code, main_name = policies[0]
    prefix = T["prefix_main"].format(month=main_month, code=main_code, name=main_name)

    if len(policies) > 1:
        others = ", ".join([f"{m} {c}ã€Š{n}ã€‹" for m, c, n in policies[1:]])
        prefix += T["prefix_others"].format(others=others)
    else:
        prefix += "\n"

    return prefix


# ===== è¿½å•åˆ¤å®šï¼ˆåªç”¨ Aï¼šèªæ„ç›¸ä¼¼åº¦ï¼‰=====
def is_followup(user_text: str, prev_emb, prev_t: float) -> bool:
    if prev_emb is None or prev_t is None:
        return False
    if (time.time() - prev_t) > FOLLOWUP_TTL_SECONDS:
        return False

    t = (user_text or "").strip()
    if not t:
        return False

    cur_emb = embed_text(t)
    sim = cosine_sim(cur_emb, prev_emb)
    return sim >= FOLLOWUP_SIM_THRESHOLD


# ===== callback Webhook æ¥æ”¶ =====
@app.route("/callback", methods=["POST"])
def callback():
    signature = request.headers.get("X-Line-Signature")
    body = request.get_data(as_text=True)

    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)

    return "OK"


# ===== handlerï¼šHR + RAG + GPT =====
@handler.add(MessageEvent, message=TextMessage)
def handle_message(event):
    user_text = (event.message.text or "").strip()
    user_id = event.source.user_id

    # ---- èªè¨€åˆ¤æ–·ï¼ˆä¸äº¤çµ¦ GPTï¼‰----
    lang = detect_lang(user_text)
    T = TEXTS.get(lang, TEXTS["zh"])

    # ---- intro é¡¯ç¤ºç¯€å¥ ----
    now = time.time()
    last = last_seen.get(user_id)
    should_show_intro = (last is None) or ((now - last) > INTRO_COOLDOWN_SECONDS)
    last_seen[user_id] = now

    # ---- è¿½å•åˆ¤å®šï¼ˆåªç”¨ Aï¼‰----
    prev = followup_state.get(user_id, {})
    prev_q = prev.get("last_q")
    prev_t = prev.get("last_t")
    prev_emb = prev.get("last_emb")

    follow = False
    if prev_q and prev_t and prev_emb:
        follow = is_followup(user_text, prev_emb, prev_t)

    # è‹¥æ˜¯è¿½å•ï¼šæŠŠæŸ¥è©¢æ”¹æˆã€Œä¸Šä¸€é¡Œ + é€™ä¸€é¡Œã€ä»¥æå‡æª¢ç´¢æº–åº¦
    # ï¼ˆæ³¨æ„ï¼šåªå½±éŸ¿æª¢ç´¢ï¼Œä¸æœƒæŠŠ prev_q é¡¯ç¤ºçµ¦ä½¿ç”¨è€…ï¼‰
    if follow:
        effective_question = f"{prev_q}\n{user_text}"
    else:
        effective_question = user_text

    # ---- å…ˆç®—ä¸€æ¬¡ embeddingï¼Œå¾Œé¢æª¢ç´¢èˆ‡è¿½å• state éƒ½å…±ç”¨ï¼ˆçœéŒ¢ï¼‰----
    q_emb = embed_text(effective_question)

    # ---- RAG æª¢ç´¢ ----
    refs = retrieve_chunks(effective_question, top_k=TOP_K, query_emb=q_emb)
    best_score = refs[0]["score"] if refs else 0.0

    # ç›¸ä¼¼åº¦ä¸è¶³ï¼šç›´æ¥æ‹’ç­”ï¼ˆé¿å…æ³›å›ç­”ï¼‰
    if best_score < SIMILARITY_THRESHOLD:
        core = T["fallback"]
        if should_show_intro:
            reply_text = f"{T['intro']}\n{core}{T['disclaimer']}"
        else:
            reply_text = f"{core}{T['disclaimer']}"

        line_bot_api.reply_message(
            event.reply_token,
            TextSendMessage(text=reply_text)
        )
        return

    # ---- å¤šä¾†æºå‰ç¶´ ----
    prefix = build_multi_source_prefix(T, refs)

    # ---- å¼•ç”¨å…§å®¹å€å¡Šï¼ˆåªçµ¦å¿…è¦ç‰‡æ®µï¼‰----
    context_block = "\n\n".join(
        [f"[{r['policy_code']}#{r['chunk_id']}] {r['text']}" for r in refs]
    )

    # ---- GPT Promptï¼ˆç¶­æŒä½ åŸæœ¬è¦å‰‡ï¼šåªèƒ½ä¾å¼•ç”¨å…§å®¹å›ç­”ï¼‰----
    prompt = f"""
ä½ æ˜¯ microSHIFT å…¬å¸çš„ HR äººè³‡åŠ©ç†ã€‚

ã€è¦å‰‡ï¼ˆéå¸¸é‡è¦ï¼‰ã€‘
1) ä½ åªèƒ½æ ¹æ“šä¸‹æ–¹ã€å¼•ç”¨å…§å®¹ã€‘å›ç­”ï¼Œç¦æ­¢ä½¿ç”¨ä¸€èˆ¬å¸¸è­˜ã€ç¶²è·¯è³‡è¨Šæˆ–æ¨æ¸¬è£œå……ã€‚
2) å¦‚æœã€å¼•ç”¨å…§å®¹ã€‘ä¸è¶³ä»¥å›ç­”ï¼Œè«‹åªå›è¦†ä¸‹åˆ—å¥å­ï¼ˆä¸è¦åŠ ä»»ä½•è£œå……ï¼‰ï¼š
{T['fallback']}

ã€èªè¨€è¦æ±‚ã€‘
{T['lang_rule']}

ã€å¼•ç”¨å…§å®¹ã€‘
{context_block}

ã€å“¡å·¥å•é¡Œã€‘
{user_text}

ã€å›ç­”æ ¼å¼è¦æ±‚ã€‘
- å°ˆæ¥­ã€æ¸…æ¥šã€ç°¡çŸ­
- å„ªå…ˆç”¨æ¢åˆ—
- è‹¥æœ‰æ¢æ–‡æ¢ä»¶/ä¾‹å¤–ï¼Œéœ€è¬›æ¸…æ¥š
"""

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯å…¬å¸å…§éƒ¨ HR Botï¼ˆåªèƒ½ä¾å¼•ç”¨å…§å®¹å›ç­”ï¼‰"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1
    )
    gpt_answer = (resp.choices[0].message.content or "").strip()

    # ---- çµ„åˆå›è¦†ï¼ˆå…è²¬è²æ˜å›ºå®šåŠ åœ¨æœ€å¾Œï¼Œä¸äº¤çµ¦ GPTï¼‰----
    if should_show_intro:
        reply_text = f"{T['intro']}\n{prefix}{T['answer_label']}{gpt_answer}{T['disclaimer']}"
    else:
        reply_text = f"{prefix}{T['answer_label']}{gpt_answer}{T['disclaimer']}"

    line_bot_api.reply_message(
        event.reply_token,
        TextSendMessage(text=reply_text)
    )

    # ---- æ›´æ–°è¿½å• state ----
    # åªåœ¨ã€Œæœ‰æˆåŠŸå›ç­”ï¼ˆéæ‹’ç­”ï¼‰ã€æ™‚æ›´æ–°ï¼›last_q è¨˜ä½ã€Œæœ‰æ•ˆå•é¡Œã€ä»¥ä¾¿ä¸‹ä¸€é¡Œè¿½å•æ‹¼æ¥
    followup_state[user_id] = {
        "last_q": effective_question if follow else user_text,
        "last_t": time.time(),
        "last_emb": q_emb,
    }


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    app.run(host="0.0.0.0", port=port)
